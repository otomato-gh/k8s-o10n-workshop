# Benchmarking Performance on K8s

- In order to assess application performance we need a way to:
    -  simulate load
    -  check application behaviour

- We want to be able to mock different real-life scenarios

- And measure resource consumption during the test

- We need a load testing tool (coming up!)

- But first let's define the types of performance tests we usually want to run:

---

## Types of Performance Tests - Average-Load

An average-load test assesses how the system performs under typical load. Typical load might be a regular day in production or an average moment.

Average-load tests simulate the number of concurrent users and requests per second that reflect average behaviors in the production environment. This type of test typically increases the throughput or VUs gradually and keeps that average load for some time. 

You should run an average-load test to:

- Assess the performance of your system under a typical load.
- Identify early degradation signs during the ramp-up or full load periods.
- Assure that the system still meets the performance standards after system changes (code and infrastructure).

---

## Types of Performance Tests - Spike

A spike test verifies whether the system survives and performs under sudden and massive rushes of utilization.

Spike tests are useful when the system may experience events of sudden and massive traffic. Examples of such events include ticket sales (Taylor Swift), product launches (PS5), broadcast ads (Super Bowl), process deadlines (tax declaration), and seasonal sales (Black Friday). Also, spikes in traffic could be caused by more frequent events such as rush hours, a particular task, or a use case.

Spike testing increases to extremely high loads in a very short or non-existent ramp-up time.

---
## Types of Performance Tests - Stress

Stress testing assesses how the system performs when loads are heavier than usual.

The load pattern of a stress test resembles that of an average-load test. The main difference is higher load. To account for higher load, the ramp-up period takes longer in proportion to the load increase. Similarly, after the test reaches the desired load, it might last for slightly longer than it would in the average-load test.

Stress tests verify the stability and reliability of the system under conditions of heavy use. Systems may receive higher than usual workloads on unusual moments such as process deadlines, paydays, rush hours, ends of the workweek, and many other behaviors that might cause frequent higher-than-average traffic.

---

## Types of Performance Tests - Soak

Soak testing is another variation of the Average-Load test. It focuses on extended periods, analyzing the following:

The system’s degradation of performance and resource consumption over extended periods.

The system’s availability and stability during extended periods.
The soak test differs from an average-load test in test duration. In a soak test, the peak load duration (usually an average amount) extends several hours and even days. Though the duration is considerably longer, the ramp-up and ramp-down periods of a soak test are the same as an average-load test.

---
## What are We Gonna Test?

- In order to run our first performance test we need an SUT (System under Test)

- To start with - we'll use `otomato/busyhttp` - it's a small Python web app that loads CPU or memory depending on the endpoint that's called.
    - Call to `/cpu` creates cpu load
    - Call to `/memory` allocates 1Mb of memory
    - Call `/memfree` frees 1Mb of memory

.lab[
```bash
  # deploy busyhttp to our cluster
  kubectl create deployment busyhttp --image=otomato/busyhttp:memory 
  kubectl expose deploy/busyhttp --port 80
```
]

---
## Measure Resource Utilization

.lab[
- Monitor pod resource usage:
```
    watch kubectl top pods -l app=busyhttp
```
- Start a network testing container:
```
    kubectl run netutils --image=otomato/net-utils "ping localhost"
```
- Create some CPU load:
```
    kubectl exec netutils -- httping -c 20 http://busyhttp/cpu
```
]

---
## Measure Resource Utilization

.lab[
- Let's run a test for memory now

- Allocate 20Mb of memory:
```
    kubectl exec netutils -- httping -c 20 http://busyhttp/memory
```
- Now release it:
```
    kubectl exec netutils -- httping -c 20 http://busyhttp/memfree
```
]

- All seems to work fine. Time to write some real tests!

---

## Load testing with k6

<img align="right" src="https://grafana.com/media/docs/k6/GrafanaLogo_k6_icon.svg">
- k6 (from GrafanaLabs) is an open-source, developer-friendly, and extensible load testing tool. 

- Using k6, you can test the reliability and performance of your application and infrastructure.

- k6 is optimized for minimal resource consumption and designed for running high-load performance tests such as **spike**, **stress**, or **soak** tests.

- Relevant to our topic: **k6-operator** allows running distributed tests on Kubernetes 

---

## Define a scenario

The test we're going to run is found in 'scripts/api-test.js' in the training repo.

It defines one ramp-up scenario that goes up to 100 virtual users in 20 seconds and then cools down for 5 seconds:

```javascript
scenarios: {
    ramping_up: {
      executor: "ramping-vus",
      stages: [
        { duration: "5s", target: 5 },
        { duration: "5s", target: 100 },
        { duration: "10s", target: 200 },
        { duration: "5s", target: 0 }, // scale down. (optional)
      ],
    },
  }
```

---

## Define Thresholds

To assess an endpoint’s performance we need to define service level objectives (SLOs). 

For example:
- 99% of requests should be successful
- 99% of requests should have a latency of 2000ms or less

To codify the SLOs we define thresholds:

```javascript
 thresholds: {
    // http errors should be less than 1%
    http_req_failed: ['rate<0.01'], 
    // 99% of requests should be below 2s
    http_req_duration: [ { threshold: 'p(99)<2000', abortOnFail: true} ], 
  },
```

`abortOnFail` on service will abort the test when we cross the threshold

---

## Installing k6

On your lab VMs `k6` should be already installed.

If running this at home - follow the official [installation instructions](https://k6.io/docs/get-started/installation/)

For Ubuntu:

```bash
sudo gpg -k
sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
sudo apt-get update
sudo apt-get install k6
```
---

## Running the test

- Port-forward to the `busyhttp` service:
.lab[
```    
    kubectl port-forward svc/busyhttp 8080:80
```
]

- And run the test:
.lab[
```
    k6 run scripts/api-test.js
```
]

- Watch pod cpu utlization grow with `kubectl top`

- After a number of iterations - the latency threshold will be crossed and the test will get aborted.

---

# Running distributed tests on k8s with the k6-operator

When running on K8s we usually want to simulate traffic coming simultaneously from multiple source. It's made possible with the `k6-operator`

.lab[
- Install the k6-operator:
```
kubectl create -f scripts/k6.yaml
```
]

---

## The TestRun CRD

- A test is described by a `TestRun` custom resource:

```yaml
apiVersion: k6.io/v1alpha1
kind: TestRun
metadata:
  name: mytest
spec:
  parallelism: 2
  script:
    configMap:
      name: mytest
```

- `parallelism` defines how many instances of k6 runners you want to create. Each instance is assigned an equal execution segment.

- `script` can be loaded from a `configMap` or a `persistentVolumeClaim`
---

## A new SUT

For this test we'll deploy a small Go app `crypter` that encrypts texts sent to it with a key that gets regenerated every 30 seconds. 

It will also decrypt that text if sent back while the key is still valid.

Meanwhile it also creates some unnecessary load on CPU and memory to make things more interesting (or silly)

.lab[
- Clone the app repo (where test definitions reside)
```
  cd ~
  git clone  https://github.com/otomato-gh/crypter.git
  cd ~/crypter
  kubectl apply -f deployment.yaml
```
]

---


## Create the PersistentVolume with the test data

- The test script and the input data (texts for encryption) have been packaged in a container image.

- We'll apply a spec that creates the pod that will mount the test data for the k6 runners to use:

.lab[
```
    cd ~/crypter/k6
    kubectl apply -f testvolume.yaml
    # wait for the pod 'testvolume' to complete
    kubectl apply -f testrun.yaml
```
]

---

## Monitor the TestRun

- Use `kubectl top` to monitor the resources of the `crypter` pod

- Once the `crypter-load` pods complete - run `kubectl logs --tail 18` on them to check test results.

.lab[
    - Limit `crypter` deployment to 200m of cpu and 50Mb of memory.

    - Rerun the test (`kubectl delete -f testrun.yaml && kubectl create -f testrun.yaml` )

    - Check how resource limits affect performance
]

---

## Discussion

- Crypter pod gets OOMKilled when under load

- Raising the memory limit will prevent it from failing but it will still get throttled under load.

- If one instance of `crypter` can't handle all the load, maybe we should try scaling out?

- But what will be our scaling criteria? 

- How do we adapt our requests and limits to support our SLOs?

- Enter VPA

